overview of computer systems
    traditional classes of computer systems
        personal computer (pc)
            designed for use by individuals, using incorporating graphical displays or other i/o elements
        server
            a computer used for running larger programs for multiple users, often simultaneously, typically accessed only via a network
        supercomputer
            a class of computers with the highest performance and cost, configured as servers
            many powerful computers working in unison
        embedded computers
            a computer inside another device, used for running one predetermined application or collection of software
                e.g. powersteering

    post-pc era
        personal mobile devices
            small wireless devices that connect to the internet
            rely on batteries for power
            software is installed through apps
                conventional examples are phones, tablets
        cloud computing
            large collections of servers that provide services over the internet
            some providers rent dynamically varying numbers of servers as a utility
        software as a service
            delivers software and data as a service over the internet, usually via a thin program such as a browser
                e.g. web search or email

    computer systems
        applications provide service
        operating system interfaces between a program and the hardware
        hardware performs tasks
            interact, calculate, store/retrieve memory

seven ideas of computer organisation
    1) abstraction
        lower level details are hidden to provide a simpler model at higher levels
    2) efficient common case
        enhancing the most common cases often improves performance more than optimising rare cases
        ironically, this is more often simple and easier to do
    3) parallelism
        computing is more efficient when done in parallel
    4) pipelining
        dear god why oh god oh fuck oh no
    5) prediction
        in some cases, it can be faster to guess the outcome and start working based on that, rather than waiting to see the outcome
        this is assuming the cost of a mistake is not too expensive, and that the guess is relatively accurate
        see xkcd/1938
    6) hierarchy of memory
        fastest memory is the most likely to be accessed
        e.g. the cache
    7) dependability via redundancy
        any system can fail, so we counteract this by two things
        being able to detect failures
        including redundant components that can take over in the case of failures

memory alignment
    computer word sizes determine the size of addresses
    if the word size is 4 bytes, the data must be at a memory address which is a multiple of 4
    if a data structure does not fit neatly as a multiple of 4, padding must be inserted to avoid an alignment fault
        e.g.
            4-byte words, however the integer being stored is only 2 bytes
            the number is represented as the integer with zeroes in front
            this is an example of how a larger word size, while allowing for more memory space, can be less efficient, as any data less than the total word size will cause data to be filled with nonsense

common compiling flow
    high level language
        - (os-specific compiler) ->
            object code
                - (combined with object files by linker) ->
                    - (turned into .exe by assembler) ->
                        executable file in 1s and 0s.

    -> java compiling flow
        java source code
            - (java compiler) ->
                byte code
                    -> runs on os-specific java vm

operating systems
    [note: embedded systems rarely have operating systems, as they don't have to be "operated"]
    main use - resource management
        runs in priveleged mode, and can stop and start user programs
