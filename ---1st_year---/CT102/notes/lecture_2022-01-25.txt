algorithmic analysis
    "counting time steps" summary
        generally, do not count i/o steps
        characteristics
            - gives a good approximation of runtime
            - requires effort in counting, and not very generic

    rate of growth analysis
        given that n will have different values for each run, it is usually the rate of growth of f(n) that we want to analyse
        e.g., with $ f(n) = n^3 + 2n + 1234 $, it is only as n gets larger that f(n) starts to get very large

        commonly used rate of growth functions
            - 1
                constant time, not n-dependent
            - log_2 n
                logarithmic, seen in binary search
            - n
                linear
            - n log_2 n
                superlinear, seen in mergesort and quicksort
            - n^2
                quadratic, typically an algorithm with a nested loop, e.g. bubblesort
            - n^3
                cubic, three nested loops
            - 2^n
                exponential, some recursive solutions or pathfinding algorithms
            - n!
                factorial, borderline unusable

            // note: those last three are huge, and impractical

        O notation (big O notation, "order")
            gives a measure of rate of growth in terms of upper and lower bounds in comparison to some standard functions
                // note: ignores coefficients and additive/multiplicative constants
                    for example:
                        - n ≡ 2n
                        - n ≡ n + 500
                        - n^2 ≡ 5n^2

            more formally:
                given f(n) for some algorithm;
                f(n) is O(g(n)) means that it is always possible to find some k such that
                f(n) <= k g(n) for n >= n_0     // large enough n

                k g(n) is an upper bound on f(n)

                examples
                    f(n) = 3n + 8
                        -> O(f(n)) is O(n)
                    f(n) = (n^2)/2 + 10n + 5
                        -> O(f(n)) is O(n^2)
                    f(n) = 2103
                        -> O(f(n)) is O(1)

            we can define similar functions for the lower bound, Ω (best case), and lower and upper bound, Θ (average case)
                // we generally consider the upper bound, O, however

            dominance relations
                O notation can be used to describe the growth rate for any particular algorithm where to coefficients and constants are of little consequence, and what's relevant for large n is the ordering of the rate of growth

                note on polynomials
                    any algorithm whose rate of growth is O(n^x) where x > 1 is said to be of polynomial time complexity
                    though impractical for all practical issues due to massive growth rate, can be useful for small n

            but what about the constants?
                k and n_0 are chosen so that for all n > n_0, k*g(n) > f(n)
                [diagram]
                similar constants can be chosen for Ω (c) and for Θ (c1 and c2)

                examples
                    f(n) = 3n + 8
                        O(f(n)) is O(n), k = 4, n_0 = 8

                    f(n) = n^4 + 100n^2 + 50
                        O(f(n)) os O(n^4), k = 2, n_0 = 11

                        /*
                        these were found just by guessing a k, and graphing f(n) vs O(f(n))
                        this constants aren't particularly relevant, however, so we foten don't need to find them
                        */


bubblesort analysis
    void bubble_sort(int array_a[], int size) {
        int i, k, temp;

        for (k = 0; k < size; k++) {
            for (i = 0; i < size - 1 - k; i ++) {
                if (array_a[i] > array_a[i+1]) {
                    // out of order, swap
                    temp = array_a[i];
                    array_a[i] = array_a[i+1];
                    array_a[i+1] = temp;
                }
            }
        }
    }
